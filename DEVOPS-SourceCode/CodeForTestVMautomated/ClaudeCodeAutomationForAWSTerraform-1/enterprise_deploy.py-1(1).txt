#!/usr/bin/env python3
"""
AWS Enterprise Auto-Deployment Tool

This script automates the deployment of enterprise infrastructure on AWS,
with robust error handling, logging, security features, and rollback capabilities.
"""

import os
import sys
import random
import string
import subprocess
import time
import json
import logging
import argparse
import tempfile
import re
import signal
import boto3
from botocore.exceptions import ClientError
from contextlib import contextmanager

# ==============================================================
# Logging Configuration
# ==============================================================

# Configure logging
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    handlers=[
        logging.FileHandler("enterprise_deploy.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ==============================================================
# Argument Parser
# ==============================================================

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Enterprise Auto-Deployment Tool for AWS')
    parser.add_argument('--region', dest='region', default=os.getenv("AWS_REGION", "us-east-1"),
                      help='AWS region to deploy resources (default: us-east-1)')
    parser.add_argument('--resource-group', dest='resource_group', default="enterprise-auto",
                      help='Resource group name for tagging (default: enterprise-auto)')
    parser.add_argument('--command-instance-type', dest='command_instance_type', 
                      default=os.getenv("COMMAND_INSTANCE_TYPE", "t2.micro"),
                      help='EC2 instance type for command instance (default: t2.micro)')
    parser.add_argument('--test-instance-type', dest='test_instance_type',
                      default=os.getenv("TEST_INSTANCE_TYPE", "t2.micro"),
                      help='EC2 instance type for test instances (default: t2.micro)')
    parser.add_argument('--ami-id', dest='ami_id',
                      default=os.getenv("AMI_ID", "ami-0c55b159cbfafe1f0"),
                      help='AMI ID for EC2 instances (default: ami-0c55b159cbfafe1f0)')
    parser.add_argument('--ssh-key-name', dest='ssh_key_name',
                      default=os.getenv("SSH_KEY_NAME", "enterprise-auto-key"),
                      help='SSH key name for EC2 instances (default: enterprise-auto-key)')
    parser.add_argument('--sns-topic-arn', dest='sns_topic_arn',
                      default=os.getenv('AWS_SNS_TOPIC_ARN', ''),
                      help='SNS topic ARN for notifications')
    parser.add_argument('--email-sender', dest='email_sender',
                      default=os.getenv('EMAIL_SENDER', ""),
                      help='Email sender address for notifications')
    parser.add_argument('--email-receiver', dest='email_receiver',
                      default=os.getenv('EMAIL_RECEIVER', ""),
                      help='Email receiver address for notifications')
    parser.add_argument('--deployment-dir', dest='deployment_dir',
                      default="./deployment",
                      help='Directory for Terraform deployments (default: ./deployment)')
    parser.add_argument('--skip-terraform', dest='skip_terraform', action='store_true',
                      help='Skip Terraform setup and deployment')
    parser.add_argument('--debug', dest='debug', action='store_true',
                      help='Enable debug logging')
    parser.add_argument('--dry-run', dest='dry_run', action='store_true',
                      help='Dry run without creating resources')
    parser.add_argument('--force', dest='force', action='store_true',
                      help='Force deployment even if resources exist')
    parser.add_argument('--rollback-on-failure', dest='rollback_on_failure', action='store_true',
                      help='Automatically rollback on failure')
    
    return parser.parse_args()

# ==============================================================
# Global Configuration
# ==============================================================

class Config:
    """Global configuration class."""
    def __init__(self, args):
        # Core settings
        self.REGION = args.region
        self.RESOURCE_GROUP_NAME = args.resource_group
        self.S3_BUCKET_NAME = "tfstate-" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))
        self.TERRAFORM_STATE_KEY = "terraform.tfstate"
        self.LIFECYCLE_POLICY_NAME = "auto-delete-90days"
        
        # EC2 Instance Types
        self.COMMAND_INSTANCE_TYPE = args.command_instance_type
        self.TEST_INSTANCE_TYPE = args.test_instance_type 
        self.AMI_ID = args.ami_id
        
        # Admin credentials
        self.SSH_KEY_NAME = args.ssh_key_name
        
        # Email Settings
        self.SNS_TOPIC_ARN = args.sns_topic_arn
        self.EMAIL_SENDER = args.email_sender
        self.EMAIL_RECEIVER = args.email_receiver
        
        # Terraform Deployment Details
        self.DEPLOYMENT_DIR = args.deployment_dir
        self.TERRAFORM_FILE = "main.tf"
        
        # Execution flags
        self.DRY_RUN = args.dry_run
        self.SKIP_TERRAFORM = args.skip_terraform
        self.FORCE = args.force
        self.ROLLBACK_ON_FAILURE = args.rollback_on_failure
        
        # Debug mode
        if args.debug:
            logger.setLevel(logging.DEBUG)
            logger.debug("Debug logging enabled")
        
        # Set up tracking for resources created by this run
        self.resources_created = {
            "S3_BUCKET": None,
            "IAM_ROLE": None,
            "IAM_POLICY": None,
            "SSH_KEY": None,
            "TERRAFORM_INSTANCE": None,
            "COMMAND_INSTANCE": None,
            "TEST_INSTANCES": [],
            "VPC": None,
            "SUBNET": None,
            "SECURITY_GROUP": None,
            "ELASTIC_IPS": []
        }
        
        # Resource IDs needed for rollback
        self.resource_ids = {}
        
        # Generate dynamic tags
        self.DYNAMIC_TAGS = self.generate_dynamic_tags()
        
        logger.debug(f"Configuration initialized: {self.__dict__}")

    def generate_dynamic_tags(self):
        """Generates dynamic tags based on environment variables (like GitHub Actions)."""
        branch = os.getenv('GITHUB_REF', 'refs/heads/unknown').split('/')[-1]
        actor = os.getenv('GITHUB_ACTOR', 'manual-run')
        
        # Add unique deployment ID
        deployment_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))

        # Determine environment tag
        env_tag = "Dev"
        if branch == "main":
            env_tag = "Production"
        elif branch.startswith("release"):
            env_tag = "Staging"

        tags = {
            "Environment": env_tag,
            "TriggeredBy": actor,
            "Branch": branch,
            "AutoCreated": "True",
            "Project": "EnterpriseAutoDeploy",
            "DeploymentID": deployment_id,
            "CreationDate": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "ResourceGroup": self.RESOURCE_GROUP_NAME
        }
        return tags

# ==============================================================
# Exception Classes
# ==============================================================

class DeploymentError(Exception):
    """Base exception for deployment errors."""
    pass

class AuthenticationError(DeploymentError):
    """Exception for AWS authentication failures."""
    pass

class ResourceCreationError(DeploymentError):
    """Exception for resource creation failures."""
    pass

class ConfigurationError(DeploymentError):
    """Exception for configuration errors."""
    pass

class TerraformError(DeploymentError):
    """Exception for Terraform operation failures."""
    pass

# ==============================================================
# Context Managers for Resource Management & Rollback
# ==============================================================

@contextmanager
def rollback_context(config):
    """Context manager for automatic rollback on exceptions."""
    try:
        yield
    except Exception as e:
        logger.error(f"Deployment failed: {str(e)}")
        if config.ROLLBACK_ON_FAILURE:
            logger.info("Initiating rollback due to failure...")
            rollback_deployment(config)
        raise

# ==============================================================
# AWS Session Management
# ==============================================================

def create_aws_session(config):
    """Create AWS session and validate credentials."""
    try:
        # Create AWS session using environment variables or AWS config
        session = boto3.Session(region_name=config.REGION)
        
        # Check if credentials are valid
        sts_client = session.client('sts')
        account_info = sts_client.get_caller_identity()
        account_id = account_info["Account"]
        
        logger.info(f"Successfully authenticated with AWS account: {account_id}")
        
        # Create all necessary service clients
        clients = {
            's3': session.client('s3'),
            'ec2': session.client('ec2'),
            'ec2_resource': session.resource('ec2'),
            'iam': session.client('iam'),
            'sns': session.client('sns'),
            'sts': sts_client,
            'cloudwatch': session.client('cloudwatch'),
            'logs': session.client('logs'),
            'kms': session.client('kms'),
        }
        
        return account_id, clients
        
    except ClientError as e:
        error_msg = f"Authentication Error: {e}"
        logger.error(error_msg)
        logger.error("Please ensure you are authenticated to AWS (e.g., via AWS CLI, environment variables, etc.)")
        raise AuthenticationError(error_msg)
    except Exception as e:
        error_msg = f"An unexpected error occurred during AWS authentication: {e}"
        logger.error(error_msg)
        raise AuthenticationError(error_msg)

# ==============================================================
# Resource Creation Functions
# ==============================================================

def create_s3_bucket(config, clients):
    """Create S3 bucket for Terraform state with proper security settings."""
    logger.info(f"Creating S3 Bucket: {config.S3_BUCKET_NAME}")
    
    if config.DRY_RUN:
        logger.info("[DRY RUN] Would create S3 bucket")
        return
    
    try:
        s3_client = clients['s3']
        
        # Check if bucket exists
        bucket_exists = False
        try:
            s3_client.head_bucket(Bucket=config.S3_BUCKET_NAME)
            bucket_exists = True
            logger.info(f"S3 bucket '{config.S3_BUCKET_NAME}' already exists.")
            
            if config.FORCE:
                logger.info(f"Force flag set, deleting existing bucket '{config.S3_BUCKET_NAME}'...")
                s3_client.delete_bucket(Bucket=config.S3_BUCKET_NAME)
                bucket_exists = False
                logger.info(f"Existing bucket '{config.S3_BUCKET_NAME}' deleted.")
            
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                pass  # Bucket doesn't exist, create it
            else:
                raise

        if not bucket_exists:
            # Validate bucket name (lowercase, no uppercase, no special chars except -)
            if not re.match(r'^[a-z0-9.-]+$', config.S3_BUCKET_NAME):
                raise ConfigurationError(f"Invalid S3 bucket name: {config.S3_BUCKET_NAME}. Must contain only lowercase letters, numbers, periods, and hyphens.")
            
            # Create bucket
            bucket_config = {}
            if config.REGION != 'us-east-1':
                bucket_config = {'LocationConstraint': config.REGION}
                
            s3_client.create_bucket(
                Bucket=config.S3_BUCKET_NAME,
                CreateBucketConfiguration=bucket_config
            )
            
            # Add bucket tags
            s3_client.put_bucket_tagging(
                Bucket=config.S3_BUCKET_NAME,
                Tagging={
                    'TagSet': [{'Key': k, 'Value': v} for k, v in config.DYNAMIC_TAGS.items()]
                }
            )
            
            logger.info(f"S3 bucket '{config.S3_BUCKET_NAME}' created and tagged.")
            config.resources_created["S3_BUCKET"] = config.S3_BUCKET_NAME
            
            # Enable versioning for soft delete capability
            s3_client.put_bucket_versioning(
                Bucket=config.S3_BUCKET_NAME,
                VersioningConfiguration={'Status': 'Enabled'}
            )
            logger.info("S3 bucket versioning enabled.")
            
            # Set up lifecycle policy (90 days expiration for non-current versions)
            lifecycle_config = {
                'Rules': [
                    {
                        'ID': config.LIFECYCLE_POLICY_NAME,
                        'Status': 'Enabled',
                        'Prefix': '',  # Apply to all objects
                        'NoncurrentVersionExpiration': {'NoncurrentDays': 90},
                        'AbortIncompleteMultipartUpload': {'DaysAfterInitiation': 7}
                    }
                ]
            }
            s3_client.put_bucket_lifecycle_configuration(
                Bucket=config.S3_BUCKET_NAME,
                LifecycleConfiguration=lifecycle_config
            )
            logger.info("S3 bucket lifecycle policy configured.")
            
            # Set up bucket policy to enforce HTTPS
            bucket_policy = {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Sid": "EnforceHTTPS",
                        "Effect": "Deny",
                        "Principal": "*",
                        "Action": "s3:*",
                        "Resource": [
                            f"arn:aws:s3:::{config.S3_BUCKET_NAME}",
                            f"arn:aws:s3:::{config.S3_BUCKET_NAME}/*"
                        ],
                        "Condition": {
                            "Bool": {
                                "aws:SecureTransport": "false"
                            }
                        }
                    }
                ]
            }
            s3_client.put_bucket_policy(
                Bucket=config.S3_BUCKET_NAME,
                Policy=json.dumps(bucket_policy)
            )
            logger.info("S3 bucket HTTPS policy configured.")
            
            # Set up default encryption
            s3_client.put_bucket_encryption(
                Bucket=config.S3_BUCKET_NAME,
                ServerSideEncryptionConfiguration={
                    'Rules': [
                        {
                            'ApplyServerSideEncryptionByDefault': {
                                'SSEAlgorithm': 'AES256'
                            }
                        }
                    ]
                }
            )
            logger.info("S3 bucket encryption enabled.")
            
            # Block public access
            s3_client.put_public_access_block(
                Bucket=config.S3_BUCKET_NAME,
                PublicAccessBlockConfiguration={
                    'BlockPublicAcls': True,
                    'IgnorePublicAcls': True,
                    'BlockPublicPolicy': True,
                    'RestrictPublicBuckets': True
                }
            )
            logger.info("S3 bucket public access blocked.")
        
        # Create empty tfstate file if it doesn't exist
        try:
            s3_client.head_object(Bucket=config.S3_BUCKET_NAME, Key=config.TERRAFORM_STATE_KEY)
            logger.info(f"{config.TERRAFORM_STATE_KEY} already exists in S3 bucket.")
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                s3_client.put_object(
                    Bucket=config.S3_BUCKET_NAME,
                    Key=config.TERRAFORM_STATE_KEY,
                    Body=b'{}',
                    ContentType='application/json'
                )
                logger.info(f"Empty {config.TERRAFORM_STATE_KEY} created in S3 bucket.")
            else:
                raise
            
    except Exception as e:
        error_msg = f"Error setting up S3 bucket: {e}"
        logger.error(error_msg)
        raise ResourceCreationError(error_msg)

def setup_iam_roles(config, clients, account_id):
    """Set up IAM roles, policies, and instance profiles."""
    logger.info("Creating IAM Role for Resource Tagging")
    
    if config.DRY_RUN:
        logger.info("[DRY RUN] Would create IAM roles and policies")
        return
    
    try:
        iam_client = clients['iam']
        
        # Define policy document for auto-tagging
        auto_tag_policy_document = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "ec2:CreateTags",
                        "ec2:DeleteTags"
                    ],
                    "Resource": "*",
                    "Condition": {
                        "StringEquals": {
                            "aws:RequestTag/AutoCreated": "True"
                        }
                    }
                }
            ]
        }
        
        # Create IAM policy
        policy_name = "AutoTaggingPolicy"
        policy_arn = f"arn:aws:iam::{account_id}:policy/{policy_name}"
        
        try:
            # Check if policy exists
            iam_client.get_policy(PolicyArn=policy_arn)
            logger.info(f"IAM policy '{policy_name}' already exists.")
            
            if config.FORCE:
                # Delete existing policy versions first
                try:
                    policy_versions = iam_client.list_policy_versions(PolicyArn=policy_arn)['Versions']
                    for version in policy_versions:
                        if not version['IsDefaultVersion']:
                            iam_client.delete_policy_version(
                                PolicyArn=policy_arn,
                                VersionId=version['VersionId']
                            )
                except ClientError:
                    # If no versions or policy doesn't exist, continue
                    pass
                
                # Create new policy version
                iam_client.create_policy_version(
                    PolicyArn=policy_arn,
                    PolicyDocument=json.dumps(auto_tag_policy_document),
                    SetAsDefault=True
                )
                logger.info(f"IAM policy '{policy_name}' updated.")
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchEntity':
                # Create policy
                response = iam_client.create_policy(
                    PolicyName=policy_name,
                    PolicyDocument=json.dumps(auto_tag_policy_document),
                    Description="Policy for automatic resource tagging"
                )
                policy_arn = response['Policy']['Arn']
                logger.info(f"IAM policy '{policy_name}' created with ARN: {policy_arn}")
                config.resources_created["IAM_POLICY"] = policy_arn
            else:
                raise
        
        # Create IAM role that can be assumed by EC2
        role_name = "AutoTaggingRole"
        try:
            # Check if role exists
            iam_client.get_role(RoleName=role_name)
            logger.info(f"IAM role '{role_name}' already exists.")
            
            if config.FORCE:
                # First detach all policies
                attached_policies = iam_client.list_attached_role_policies(RoleName=role_name)
                for policy in attached_policies.get('AttachedPolicies', []):
                    iam_client.detach_role_policy(
                        RoleName=role_name,
                        PolicyArn=policy['PolicyArn']
                    )
                
                # Then delete the role
                iam_client.delete_role(RoleName=role_name)
                logger.info(f"Deleted existing IAM role '{role_name}'")
                # Role will be recreated below
                raise ClientError({'Error': {'Code': 'NoSuchEntity'}}, 'operation')
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchEntity':
                # Create role
                assume_role_policy_document = {
                    "Version": "2012-10-17",
                    "Statement": [
                        {
                            "Effect": "Allow",
                            "Principal": {"Service": "ec2.amazonaws.com"},
                            "Action": "sts:AssumeRole"
                        }
                    ]
                }
                
                response = iam_client.create_role(
                    RoleName=role_name,
                    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),
                    Description="Role for automatic resource tagging",
                    Tags=[{'Key': k, 'Value': v} for k, v in config.DYNAMIC_TAGS.items()]
                )
                
                role_arn = response['Role']['Arn']
                config.resources_created["IAM_ROLE"] = role_arn
                
                # Attach policy to role
                iam_client.attach_role_policy(
                    RoleName=role_name,
                    PolicyArn=policy_arn
                )
                
                logger.info(f"IAM role '{role_name}' created with ARN: {role_arn} and policy attached.")
            else:
                raise
        
        # Create instance profile for the role
        instance_profile_name = "AutoTaggingProfile"
        try:
            iam_client.get_instance_profile(InstanceProfileName=instance_profile_name)
            logger.info(f"Instance profile '{instance_profile_name}' already exists.")
            
            # If force is set, recreate the profile
            if config.FORCE:
                # First, remove all roles from the profile
                try:
                    profile = iam_client.get_instance_profile(InstanceProfileName=instance_profile_name)
                    for role in profile.get('InstanceProfile', {}).get('Roles', []):
                        iam_client.remove_role_from_instance_profile(
                            InstanceProfileName=instance_profile_name,
                            RoleName=role['RoleName']
                        )
                except Exception as e:
                    logger.warning(f"Error cleaning up instance profile: {e}")
                
                # Then delete and recreate
                iam_client.delete_instance_profile(InstanceProfileName=instance_profile_name)
                logger.info(f"Deleted existing instance profile '{instance_profile_name}'")
                # Profile will be recreated below
                raise ClientError({'Error': {'Code': 'NoSuchEntity'}}, 'operation')
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchEntity':
                # Create instance profile
                iam_client.create_instance_profile(
                    InstanceProfileName=instance_profile_name,
                    Tags=[{'Key': k, 'Value': v} for k, v in config.DYNAMIC_TAGS.items()]
                )
                
                # Add role to instance profile
                iam_client.add_role_to_instance_profile(
                    InstanceProfileName=instance_profile_name,
                    RoleName=role_name
                )
                
                logger.info(f"Instance profile '{instance_profile_name}' created and role attached.")
            else:
                raise
        
        # Return the profile name for EC2 instances
        return instance_profile_name
            
    except Exception as e:
        if isinstance(e, ResourceCreationError):
            raise
        error_msg = f"Error setting up IAM roles: {e}"
        logger.error(error_msg)
        # Continue execution, as IAM failures shouldn't stop the entire deployment
        logger.warning("Continuing deployment despite IAM setup failure")
        return None

def setup_ssh_key(config, clients):
    """Create SSH key pair if it doesn't exist."""
    logger.info(f"Checking for SSH key pair: {config.SSH_KEY_NAME}")
    
    if config.DRY_RUN:
        logger.info("[DRY RUN] Would check/create SSH key pair")
        return
    
    try:
        ec2_client = clients['ec2']
        
        # Check if key pair exists
        try:
            ec2_client.describe_key_pairs(KeyNames=[config.SSH_KEY_NAME])
            logger.info(f"SSH key pair '{config.SSH_KEY_NAME}' already exists.")
            
            if config.FORCE:
                logger.info(f"Force flag set. Deleting existing key pair '{config.SSH_KEY_NAME}'")
                ec2_client.delete_key_pair(KeyName=config.SSH_KEY_NAME)
                raise ClientError({'Error': {'Code': 'InvalidKeyPair.NotFound'}}, 'operation')
                
        except ClientError as e:
            if e.response['Error']['Code'] == 'InvalidKeyPair.NotFound':
                # Create key pair
                response = ec2_client.create_key_pair(
                    KeyName=config.SSH_KEY_NAME,
                    TagSpecifications=[{
                        'ResourceType': 'key-pair',
                        'Tags': [{'Key': k, 'Value': v} for k, v in config.DYNAMIC_TAGS.items()]
                    }]
                )
                
                # Save the private key to a file
                private_key = response['KeyMaterial']
                key_file_path = f"./{config.SSH_KEY_NAME}.pem"
                
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(os.path.abspath(key_file_path)), exist_ok=True)
                
                with open(key_file_path, 'w') as key_file:
                    key_file.write(private_key)
                
                # Set correct permissions
                os.chmod(key_file_path, 0o400)
                
                config.resources_created["SSH_KEY"] = config.SSH_KEY_NAME
                logger.info(f"SSH key pair '{config.SSH_KEY_NAME}' created and saved to {key_file_path}")
            else:
                raise
        
    except Exception as e:
        error_msg = f"Error setting up SSH key pair: {e}"
        logger.error(error_msg)
        raise ResourceCreationError(error_msg)

def generate_terraform_files(config):
    """Generate Terraform deployment files with error handling."""
    logger.info("Generating Terraform Deployment Files...")
    
    if config.DRY_RUN:
        logger.info("[DRY RUN] Would generate Terraform files")
        return
    
    try:
        # Create deployment directory with intermediate directories if needed
        os.makedirs(config.DEPLOYMENT_DIR, exist_ok=True)
        
        # Generate main.tf content with retry logic for DNS resolution
        terraform_main_content = f"""
terraform {{
  backend "s3" {{
    bucket         = "{config.S3_BUCKET_NAME}"
    key            = "{config.TERRAFORM_STATE_KEY}"
    region         = "{config.REGION}"
    encrypt        = true
    # Error handling for DNS resolution
    dynamodb_table = "" # No locking table to avoid additional dependency
    skip_region_validation = true
    skip_credentials_validation = true
    skip_metadata_api_check = true
  }}
}}

provider "aws" {{
  region = "{config.REGION}"
  
  # Add default tags to all resources
  default_tags {{
    tags = {{
      Environment  = "{config.DYNAMIC_TAGS.get('Environment', 'Unknown')}"
      TriggeredBy  = "{config.DYNAMIC_TAGS.get('TriggeredBy', 'Unknown')}"
      Branch       = "{config.DYNAMIC_TAGS.get('Branch', 'Unknown')}"
      AutoCreated  = "True"
      DeploymentID = "{config.DYNAMIC_TAGS.get('DeploymentID', 'Unknown')}" 
      Project      = "EnterpriseAutoDeploy"
      ResourceGroup = "{config.RESOURCE_GROUP_NAME}"
      CreationDate = "{config.DYNAMIC_TAGS.get('CreationDate', 'Unknown')}"
    }}
  }}
  
  # Improve error handling with retries
  max_retries = 5
  retry_mode  = "standard"
}}

# Create a VPC
resource "aws_vpc" "main" {{
  cidr_block           = "10.0.0.0/16"
  enable_dns_support   = true
  enable_dns_hostnames = true
  
  tags = {{
    Name = "enterprise-vpc"
  }}
  
  lifecycle {{
    create_before_destroy = true
  }}
}}

# Create an internet gateway
resource "aws_internet_gateway" "gw" {{
  vpc_id = aws_vpc.main.id
  
  tags = {{
    Name = "enterprise-igw"
  }}
  
  depends_on = [aws_vpc.main]
}}

# Create a subnet
resource "aws_subnet" "main" {{
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "{config.REGION}a"
  map_public_ip_on_launch = true
  
  tags = {{
    Name = "enterprise-subnet"
  }}
  
  depends_on = [aws_vpc.main]
}}

# Create a route table
resource "aws_route_table" "rt" {{
  vpc_id = aws_vpc.main.id
  
  route {{
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }}
  
  tags = {{
    Name = "enterprise-route-table"
  }}
  
  depends_on = [aws_vpc.main, aws_internet_gateway.gw]
}}

# Associate subnet with route table
resource "aws_route_table_association" "a" {{
  subnet_id      = aws_subnet.main.id
  route_table_id = aws_route_table.rt.id
  
  depends_on = [aws_subnet.main, aws_route_table.rt]
}}

# Create a security group
resource "aws_security_group" "sg" {{
  name        = "enterprise-sg"
  description = "Allow SSH and common inbound traffic"
  vpc_id      = aws_vpc.main.id
  
  ingress {{
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "SSH access"
  }}
  
  ingress {{
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    description = "HTTP access"
    cidr_blocks = ["0.0.0.0/0"]  # Allow HTTP from anywhere
  }}
  # Allow outbound traffic
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"  # All protocols
    cidr_blocks = ["0.0.0.0/0"]  # Allow all outbound traffic
  }
}}